# ASL_Science2Speech ğŸ¤ŸğŸ”Š

A real-time deep learning project that converts American Sign Language (ASL) hand gestures into audible speech. It integrates image-based, skeletal-based, and multi-modal models for accurate recognition and uses Text-to-Speech (TTS) to speak predictions.

---

## ğŸ“Œ Features

- ğŸ¥ Real-time webcam input
- ğŸ§  Deep learning models (image, skeletal, multi-modal)
- ğŸ–ï¸ Hand tracking using [MediaPipe Hands](https://google.github.io/mediapipe/solutions/hands)
- ğŸ”Š Instant speech output using `pyttsx3`
- ğŸ§ª Fully functional training + preprocessing pipeline
- ğŸ—‚ï¸ Modular architecture for experimentation

---

## ğŸ—‚ï¸ Project Structure

ASL_Science2Speech/
â”œâ”€â”€ Final codes/ # Real-time model execution scripts
â”‚ â”œâ”€â”€ baseline do not touch.py
â”‚ â”œâ”€â”€ skaletal do not touch.py
â”‚ â”œâ”€â”€ multi modal do not touch.py
â”‚ â””â”€â”€ final3_combined.py
â”‚
â”œâ”€â”€ model files/ # Model training & preprocessing
â”‚ â”œâ”€â”€ Baseline/
â”‚ â”‚ â”œâ”€â”€ model training.py
â”‚ â”‚ â”œâ”€â”€ preprocess_asl.py
â”‚ â”‚ â””â”€â”€ splitting.py
â”‚ â”œâ”€â”€ skeletal/
â”‚ â”‚ â”œâ”€â”€ skeletal_extraction.py
â”‚ â”‚ â””â”€â”€ skeletal_training.py
â”‚ â””â”€â”€ multi modal/
â”‚ â”œâ”€â”€ multi_modal_preprocessing.py
â”‚ â””â”€â”€ multi_modal_training.py
â”‚
â”œâ”€â”€ keypoints_csv files/ # Precomputed skeletal keypoints
â”‚ â””â”€â”€ skeletal_keypoints.csv
â”‚
â”œâ”€â”€ Models/ # Trained models (.h5)
â”‚ â”œâ”€â”€ baseline/
â”‚ â”œâ”€â”€ skeletal/
â”‚ â””â”€â”€ multi modal/
â”‚
â””â”€â”€ asl_dataset.zip # Raw or augmented dataset archive

yaml
Copy
Edit

---

## ğŸš€ How to Run (Real-Time Inference)

Make sure your webcam is connected.

### ğŸ” All Models Combined (Skeletal + Image + Multi-Modal)


python "Final codes/final3_combined.py"
ğŸ§  Run Individually:
bash
Copy
Edit
# Baseline image model
python "Final codes/baseline do not touch.py"

# Skeletal keypoints model
python "Final codes/skaletal do not touch.py"

# Multi-modal model (image + keypoints)
python "Final codes/multi modal do not touch.py"
ğŸ§ª Model Training Pipeline
You can train your own models from scratch.

ğŸ”§ 1. Preprocess + Augment Dataset
bash
Copy
Edit
python model files/Baseline/preprocess_asl.py
âœ‚ï¸ 2. Split into Train/Val/Test
bash
Copy
Edit
python model files/Baseline/splitting.py
ğŸ‹ï¸ 3. Train Models
bash
Copy
Edit
# Baseline (MobileNetV2)
python model files/Baseline/model training.py

# Skeletal
python model files/skeletal/skeletal_extraction.py
python model files/skeletal/skeletal_training.py

# Multi-Modal
python model files/multi modal/multi_modal_preprocessing.py
python model files/multi modal/multi_modal_training.py
ğŸ§° Dependencies
Install them with:

bash
Copy
Edit
pip install -r requirements.txt
Key Libraries:
tensorflow

opencv-python

mediapipe

pyttsx3

numpy, pandas, scikit-learn

ğŸ“Š Models Used
Model Type	Input	Description
Baseline	Image only (224Ã—224)	MobileNetV2-based classifier
Skeletal	2D keypoints (x, y only)	MLP trained on hand landmarks
Multi-Modal	Image + Keypoints	CNN + Dense hybrid fusion architecture

ğŸ“ Acknowledgements
ASL Alphabet Dataset

MediaPipe Hands

TensorFlow, OpenCV, and Pyttsx3 teams

ğŸ“£ Author
Akshit Sharma
ğŸ”— GitHub: Akshit7103

ğŸ“ License
This project is for educational and research purposes only.

python
Copy
Edit

---

Let me know if you'd like me to generate:

- `requirements.txt` from your environment
- A `setup.sh` or `.bat` script for quick setup
- GitHub repository badges (build, license, etc.)

You're all set to publish and show this off now!
